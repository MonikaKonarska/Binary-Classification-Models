---
title: "Machine learning models in R"
output: 
  html_document:
  toc: true
  toc_float: true
---

```{css, echo = FALSE}
h1.title {
  color: #003333;
}
h1 {
  color: #003333;
}
h2 {
  color: #660000;
}
h3 {
  color: #660000;
}
h4 {
  color: #FF9933;
}
```

```{r loadInformations, message = FALSE, warning = FALSE, echo = FALSE}
library(cowplot)
library(ggplot2)
library(formattable)
library(ggthemes)
library(kableExtra)
library(dplyr)
library(caret)
library(ROCR)
library(pROC)

source(file.path(getwd(), "..", "definition_of_file_paths.R"))
source(file.path(functionPath, "functions.R"))
source(file.path(functionPath, "functionsToVisualize.R"))
source(file.path(functionPath, "remove_outliers.R"))
source(file.path(functionPath, "..", "3_feature_engineering.R"))
```
<br/>
<br/>

## Dataset description

Dataset used from **Kaggle Lending Club**. To import dataset it is needed to download a file <em>loan.csv</em> from this [website](https://www.kaggle.com/wendykan/lending-club-loan-data). This file contains loan data for all loans issued through the 2007-2015. The file is a matrix of about 890 thousand observations and 75 variables. 

Let's check how the number of bad customers has changed over time.

```{r dataset, message = FALSE, warning = FALSE, echo = FALSE}
cowplot::plot_grid(plotlist = listOfPlotsTimeSeries)
```
<br/>
<br/>

## Cleaning data

Summary of the data structure:

```{r data_cleaning_informations, message = FALSE, warning = FALSE, echo = FALSE}
load(file.path(folderToSavecalculations, "structureData.RData"))

kable(structureData) %>%
  kable_styling() %>%
  scroll_box(width = "80%", height = "400px")
```


To clean data, these conditions were used:

- delete variable if missing values are more than 80%,
- delete variable (type: character) if a number of unique values are more than 30.

Defined variables (`variablesFromFeature`, `variablesAnotherToDelete`) that were not used in the modeling process because they were not significant in the prediction.

```{r variables_not_used, message = FALSE, warning = FALSE, echo = TRUE, eval=FALSE, out.width = "50%"}
structureData <<- describe_variables(dataToTranTestValid)
levelOfNA <- 0.80
maxUniqueValues <- 30
  
variablesWithalotNA         <- structureData$variable[which(structureData$p_numberOfNa >= levelOfNA )]
variablesWithalotUniqueText <- structureData$variable[ which(structureData$type == 'character' & structureData$uniqueValues > maxUniqueValues)]
  
variablesWithOneValue       <- structureData$variable[which(structureData$uniqueValues == 1 )]
variablesToRemove           <- c(variablesWithalotNA, variablesWithalotUniqueText, variablesWithOneValue)
variablesFromFeature        <<- c("sub_grade", "grade", "int_rate", "installment", "total_pymnt", "total_pymnt_inv", "total_rec_prncp", "total_rec_int", "total_rec_late_fee", "recoveries", "collection_recovery_fee", "last_pymnt_amnt", "initial_list_status")

variablesAnotherToDelete    <<- c("id", "member_id", "issue_d", "loan_status", "funded_amnt_inv", "term", "verification_status", "funded_amnt", "group", "last_credit_pull_d", "collections_12_mths_ex_med", "acc_now_delinq")
```

<br/>
<br/>

## Feature engineering

```{r new_features, message = FALSE, warning = FALSE, echo = TRUE, eval=FALSE, out.width = "50%"}
dataTrainWithNewFeatures <- data %>%
    mutate(if_delinq_in2yrs = factor(case_when(delinq_2yrs == 0 ~ 0, TRUE ~ 1), levels = c(0, 1)),
           
           if_delinq_in_last_year = factor(case_when(is.na(mths_since_last_delinq) ~ "LACK",
                                               mths_since_last_delinq <= 12 ~ "1",
                                               TRUE ~ "0")),
           if_delinq_ever = factor(case_when(is.na(mths_since_last_delinq) ~ 0, TRUE ~ 1)),
           if_inq_in_last_6moths = factor(case_when(inq_last_6mths == "0" ~ 0, TRUE ~ 1)),
           inq_last_6mths_grouped = factor(case_when(inq_last_6mths >= 4 ~ "4+",
                                                      TRUE ~ as.character(inq_last_6mths)), levels = c("0","1","2","3","4+")),
           if_ever_pub_rec = factor(case_when(pub_rec == 0 ~ 0, TRUE ~1), levels = c(0,1)),
           if_purpose_debt_consolidation = factor(case_when(purpose == 'debt_consolidation' ~ 1, TRUE ~ 0), levels = c(0,1)),
           if_employment_more_10years = factor(case_when(emp_length == '10+ years' ~ 1, TRUE ~ 0)),
           month = substr(x = funded_loan_date, start = 1, stop = 7),
           month = factor(month, levels = sort(unique(month))),
           number_of_quarter = lubridate::quarter(funded_loan_date),
           year_of_funded_loan = lubridate::year(funded_loan_date),
           quarter = paste(year_of_funded_loan, "Q", number_of_quarter, sep = "")) %>%
    dplyr::select(-c(funded_loan_date, earliest_cr_line_date, last_credit_pull_date, number_of_quarter, year_of_funded_loan, delinq_2yrs, mths_since_last_major_derog, mths_since_last_delinq, inq_last_6mths, pub_rec))
  
  dataTrainWithNewFeatures <- changeCharacter2FactorVariableWithLackGroup(data = dataTrainWithNewFeatures,
                                                                          typeOfLack = "LACK")

```

<br/>
<br/>

## Models building

## Logistic Regression

### Using Information value (IV) and weight of evidence (Woe)   

<br/>

#### **Feature selecion using the Information value (IV) and the weight of evidence (Woe)**

Using measure <em>Information value</em> you can select important variables in a predictive model.
<p>This measure helps to rank variables on the basis of their importance.</p> 

Information values for all variables from the <em>Lending Club dataset</em> are as follows:

```{r iv, message = FALSE, warning = FALSE, echo = FALSE}
    cowplot::plot_grid(results$iv_for_all_variables_plot) 
```


When information value is less than 0.02 means that it is not useful for prediction. For 11 variables from the dataset invormation value is greater than 0.02:

```{r table_of_iv, message = FALSE, warning = FALSE, echo = FALSE}
    table_of_iv <- results$information_table$Summary %>%
      mutate(IV = round(IV, 5),
          `Is it a useful variable?` = IV)
    rownames(table_of_iv) <- c()
    
    formattable(table_of_iv,
                align = c("l","r"),
                list(IV = color_bar("lightgreen"),
                     `Is it a useful variable?` = formatter("span", x ~ icontext(ifelse(x >0.02, "ok", "remove"),
                                                                                     ifelse(x>0.02, "Yes", "No")),
                                                                style = x ~ style(color = ifelse(x>0.02, "green", "red")))))

```

The weight of evidence describes as a measure of the separation of good and bad borrowers. Bad borrowers are customers who defaulted on a loan. Instead of good borrowers are customers who paid back a loan. Analyzing results from these <em>Woe plots</em> It is important to remember that:

- The WOE should be monotonic i.e. either growing or decreasing with the bins,
- Woe can handle missing values as missing values can be binned separately. It is natural that can be Woe value for bin 'NA' of a variable.

To read more about terminology of <em>IV</em> and <em>Woe</em> click [here](http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/). 
Here some plots about Woe for each independent variable:
</br>

```{r woe, message = FALSE, warning = FALSE, echo = FALSE}
results$plotsOfWoe <- results$plotsOfWoe[which(names(results$plotsOfWoe) !="month")]

    cowplot::plot_grid(plotlist =  results$plotsOfWoe[1:3], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[4:6], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[7:9], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[10:12], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[13:15], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[16:18], nrow = 1)
    cowplot::plot_grid(plotlist =  results$plotsOfWoe[19:21], nrow = 1)
    
```

We can choose the right prospective variables to build a logistic regression model when we use information about measure IV and Woe (is monotonic or not for a variable). These variables fulfill the mentioned conditions:

```{r bins_iv, message = FALSE, warning = FALSE, echo = TRUE}
listOfSeletedVariables$continuous
listOfSeletedVariables$discrete
```
</br>

Lets see how <em>IV</em> measure changes over time. How strong this measure is different over time ? 
There are times in which a decrease in value <em>IV</em> is noticeable. It can be seen that the decline usually occurs from 2012 Q4. Greater <em>IV</em> stabilization has been visible since 2013 Q2.

```{r plots_iv_in_time_for_choosen_variables, message = FALSE, warning = FALSE, echo = FALSE, out.width = "70%"}
cowplot::plot_grid(plotlist = plots_iv_in_time_for_choosen_variables)
```
</br>


Lets see how the <em>IV</em> measure depends on the number of bins for each selected, continuous variable.
It can be seen that the <em>IV</em> value for the number of bins less than or equal to 3 has a small IV value. However, these changes for the next number of the bins and the <em>IV</em> measure increases.
</br>
```{r plots_of_iv_and_bins, message = FALSE, warning = FALSE, echo = FALSE, out.width = "70%"}
cowplot::plot_grid(plotlist = plots_of_iv_and_bins)
```

</br>
</br>

#### **Coding selected variables using the value of the weight of evidence (Woe)**

Weight of evidence (WOE) coding of a nominal or discrete variable is widely used when preparing predictors for usage in binary logistic regression models. To build logistic regression model we can use Woe information. Here the example for <em>tot_cur_bal</em> variable and the results of coding.

```{r example_code_woe, message = FALSE, warning = FALSE, echo = FALSE }
informationTableSelectedVariables$tot_cur_bal %>%
  formattable(list(`WOE` = color_bar("lightgreen")))
```


```{r code_woe, message = FALSE, warning = FALSE, echo = FALSE }
dataSetWithVariablesCodedWoe <- assignWoeValueInVariables(variables_name = continuous_variables,
                                                          listOfWoe = informationTableSelectedVariables,
                                                          data = dataTrainWithNewFeatures)

dataSetWithVariablesCodedWoe[["home_ownership_woe"]] <- as.factor(
  with(dataTrainWithNewFeatures, dplyr::case_when(home_ownership == "MORTGAGE" ~ -0.19739742,
                                                  home_ownership == "OWN"  ~ 0.07469287,
                                                  home_ownership == "RENT" ~ 0.17447816)))
```


```{r table_woe_variables, message = FALSE, warning = FALSE, echo = FALSE}
dataSetWithVariablesCodedWoe[["target"]] <- dataTrainWithNewFeatures$target
kable(head(dataSetWithVariablesCodedWoe, 5)) %>%
  kable_styling() %>%
  scroll_box(width = "100%", height = "300px")
```
</br>

The table of percentage of the target in train dataset:
```{r percentage_of_target, message = FALSE, warning = FALSE, echo = FALSE}
prop.table(table(dataSetWithVariablesCodedWoe$target, useNA = "ifany"))
```

</br>
</br>

```{r train_dataset, message = FALSE, warning = FALSE, echo = FALSE}
dataSetWithVariablesCodedWoe <- dataSetWithVariablesCodedWoe %>% dplyr::select(contains("_woe"))
dataSetWithVariablesCodedWoe[["target"]] <- dataTrainWithNewFeatures$target
dataSetWithVariablesCodedWoe <- dataSetWithVariablesCodedWoe %>%
  mutate_at(c("annual_inc_woe", "dti_woe", "tot_cur_bal_woe", "total_rev_hi_lim_woe", "revol_util_woe", "total_acc_woe", "home_ownership_woe"),
            funs(as.numeric(as.character(.))))
```

</br>

#### **Correlations between independent variables**

</br>
Corrplot:

```{r corrplot, message = FALSE, warning = FALSE, echo = FALSE}
names_of_independent_variables_coded_woe <- names(dataSetWithVariablesCodedWoe)
names_of_independent_variables_coded_woe <- names_of_independent_variables_coded_woe[which(!names_of_independent_variables_coded_woe %in% c("target"))]

correlations_independent_variables_coded_woe <- round(cor(dataSetWithVariablesCodedWoe[, names_of_independent_variables_coded_woe]),3)
corrplot::corrplot(correlations_independent_variables_coded_woe)
```

</br>
Correlation matrix:

```{r corrmatrix, message = FALSE, warning = FALSE, echo = FALSE }
correlations_independent_variables_coded_woe %>%
  as.data.frame() %>%
  formattable(list(area(col = 1:ncol(correlations_independent_variables_coded_woe)) ~ color_bar("orange")))
```

</br>

Using the function `caret::findCorrelation` it is possible to find quickly highly correlated variables. 
*This function searches through a correlation matrix and returns a vector of integers corresponding to columns to remove to reduce pair-wise correlations.* [More informations](https://www.rdocumentation.org/packages/caret/versions/6.0-84/topics/findCorrelation)

The cutoff of correaltions is 0.65. No one independent numeric variable should be removed from the training dataset (independent variables coded woe).

```{r find_Correlations_woe, message = FALSE, warning = FALSE, echo = TRUE}
correlation_matrix_numeric_variables <- cor(dataSetWithVariablesCodedWoe[, names_of_independent_variables_coded_woe], use = "complete.obs")
highly_correlations_variables_woe <- caret::findCorrelation(correlation_matrix_numeric_variables, names = TRUE, cutoff = 0.65)
```

</br>
</br>

## Another logistic regression models

The first logistic regression model used all the choosen independent variables (annual_inc_woe, dti_woe, total_rev_hi_lim_woe, revol_util_woe, total_acc_woe, home_ownership_woe)
Letâ€™s name it *full_model_lr_woe*. Summary of this model below:

```{r logisticRegressionWithWoeModel, message = FALSE, warning = FALSE, echo = TRUE}
full_model_lr_woe <- glm(target~.,
                         data = dataSetWithVariablesCodedWoe,
                         family = binomial(link = "logit"))

summary(full_model_lr_woe)
```


```{r prepare_test_valid_samples, message = FALSE, warning = FALSE, echo = FALSE}
load(file = file.path(dataPath, "dataToModeling.RData"))
dataTest  <- featurEngineering(data = dataTest)
dataValid <- featurEngineering(data = dataValid)
dataTest <- dataTest[!is.na(dataTest$revol_util), ]
dataValid <- dataValid[!is.na(dataValid$revol_util), ]

dataTestWithVariablesCodedWoe <- assignWoeValueInVariables(variables_name = continuous_variables,
                                                           listOfWoe = informationTableSelectedVariables,
                                                           data = dataTest)
dataTestWithVariablesCodedWoe[["home_ownership_woe"]] <- as.factor(with(dataTestWithVariablesCodedWoe,
                                                                        dplyr::case_when(
                                                                          home_ownership == "MORTGAGE" ~ -0.19739742,
                                                                          home_ownership == "OWN"  ~ 0.07469287,
                                                                          home_ownership == "RENT" ~ 0.17447816)))
dataTestWithVariablesCodedWoe <- dataTestWithVariablesCodedWoe[which(!is.na(dataTestWithVariablesCodedWoe$target)),]
dataTestWithVariablesCodedWoe <- dataTestWithVariablesCodedWoe %>%
  mutate_at(c("annual_inc_woe", "dti_woe", "tot_cur_bal_woe", "total_rev_hi_lim_woe", "revol_util_woe", "total_acc_woe", "home_ownership_woe"),
            funs(as.numeric(as.character(.))))


dataValidWithVariablesCodedWoe <- assignWoeValueInVariables(variables_name = continuous_variables,
                                                            listOfWoe = informationTableSelectedVariables,
                                                            data = dataValid)
dataValidWithVariablesCodedWoe[["home_ownership_woe"]] <- as.factor(with(dataValidWithVariablesCodedWoe,
                                                                         dplyr::case_when(
                                                                           home_ownership == "MORTGAGE" ~ -0.19739742,
                                                                           home_ownership == "OWN"  ~ 0.07469287,
                                                                           home_ownership == "RENT" ~ 0.17447816)))
dataValidWithVariablesCodedWoe <- dataValidWithVariablesCodedWoe[which(!is.na(dataValidWithVariablesCodedWoe$target)),]
dataValidWithVariablesCodedWoe <- dataValidWithVariablesCodedWoe %>%
  mutate_at(c("annual_inc_woe", "dti_woe", "tot_cur_bal_woe", "total_rev_hi_lim_woe", "revol_util_woe", "total_acc_woe", "home_ownership_woe"),
            funs(as.numeric(as.character(.))))

train_predictions <- dataSetWithVariablesCodedWoe['target'] 
test_predictions  <- dataTestWithVariablesCodedWoe['target']
valid_predictions <- dataValidWithVariablesCodedWoe['target']
```

</br>

### Stepwise to choose the best logistic regression model (variables coding woe)

```{r stepwise_model_lr_woe, message = FALSE, warning = FALSE, echo = TRUE, include = FALSE}
null_model_lr_woe <- glm(target ~ 1, data = dataSetWithVariablesCodedWoe, family = binomial(link = "logit"))
step_model_lr_woe <- step(null_model_lr_woe, scope = list(lower = null_model_lr_woe, upper = full_model_lr_woe),
                          direction = "forward")
```

Which one is the best model (coding Woe variables) ?

```{r the_best_model_woe, message = FALSE, warning = FALSE, echo = TRUE}
summary(step_model_lr_woe)
```
</br>

Lets evaluate the model performance. To do that lets use the testing and validation datasets to predict the target variable on unseen (test, valid) data. Predictions of the target variable are probabilities of default. Using this information allows us to evaluate the chosen threshold.

### ROC curve

```{r the_best_model_woe_plots, message = FALSE, warning = FALSE, echo = FALSE, out.width = "50%"}
train_predictions$step_model_lr_woe_pred <- predict(full_model_lr_woe, dataSetWithVariablesCodedWoe,   type = "response")
test_predictions$step_model_lr_woe_pred  <- predict(full_model_lr_woe, dataTestWithVariablesCodedWoe,  type = "response")
valid_predictions$step_model_lr_woe_pred <- predict(full_model_lr_woe, dataValidWithVariablesCodedWoe, type = "response")

plot.roc(train_predictions$target, train_predictions$step_model_lr_woe_pred, col = "red", main="ROC training data",
         percent = TRUE, print.auc = TRUE)

plot.roc(test_predictions$target, test_predictions$step_model_lr_woe_pred, col = "red", main="ROC testing data",
         percent = TRUE, print.auc = TRUE)

plot.roc(valid_predictions$target, valid_predictions$step_model_lr_woe_pred, col = "red", main="ROC validation data",
         percent = TRUE, print.auc = TRUE)
```
</br>

In the current case, both distributions are slight skewed to the left: 

```{r plot_score_glm_woe, message = FALSE, warning = FALSE, echo = FALSE, out.width = "50%"}
ggplot(train_predictions, aes(step_model_lr_woe_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The training dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()

ggplot(test_predictions, aes(step_model_lr_woe_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The testing dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()

ggplot(valid_predictions, aes(step_model_lr_woe_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The validation dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()

```
</br>

### Cut off point

When using the model to actually predict who is the bad customer a *cut-off point* has to be defined.
Confusion Matrix allows the easier observation of some classification properties (e.g. cost incurred in case of incorrect classification).


The cut-off point of 0.25 seems to be the best.
</br>


```{r confusionMatrix_glm, message = FALSE, warning = FALSE, echo = FALSE}
accuracy_info <- show_accuracy_for_cutoffs(train = train_predictions,
                                           test = test_predictions,
                                           valid = valid_predictions,
                                           predict = "step_model_lr_woe_pred",
                                           actual = "target",
                                           cutoffmax  = 0.35,
                                           cutoffmin = 0.1)
accuracy_info$plot
```


```{r cm, message = FALSE, warning = FALSE, echo = FALSE}
confusionMatrix(table( as.numeric( train_predictions$step_model_lr_woe_pred > 0.25 ), train_predictions$target ))
confusionMatrix(table( as.numeric( test_predictions$step_model_lr_woe_pred > 0.25 ), test_predictions$target ))
confusionMatrix(table( as.numeric( valid_predictions$step_model_lr_woe_pred > 0.25 ), valid_predictions$target ))
```

It's not possible to reduce the False Negative  to 0.25 However at a cost of a few extra False Positives can be found.
The Kappa index of agreement tells how much better, or worse, this classifier is than what would be expected by random chance.
Kappa index for this logistic regression classifier means that this classifier is about:

- 11% better than a random assignment of cases to the various classes in the training dataset,
- 11% better than a random assignment of cases to the various classes in the testing dataset, 
- 12% better than a random assignment of cases to the various classes in the valid dataset.

This not sounds good. 

<br/>
<br/>

## Another logistic regression models (without woe)

Corrplot all numeric independent variables in the training dataset:

```{r correlations_all_numeric_variables, message = FALSE, warning = FALSE, echo = FALSE}
numeric_all_variables_names <- dataTrainWithNewFeatures %>%
  sapply(is.numeric) %>% 
  which() %>% 
  names()

corr_matrix_numeric_var <- cor(dataTrainWithNewFeatures[, numeric_all_variables_names], use = "complete.obs")

corrplot::corrplot(corr_matrix_numeric_var, method = "pie", type = "upper")
```


```{r variables_with_higher_correlation,  message = FALSE, warning = FALSE, echo = TRUE}
highly_correlated  <- caret::findCorrelation(corr_matrix_numeric_var, names = TRUE, cutoff = 0.65)
highly_correlated
```


```{r other_glm, message = FALSE, warning = FALSE, echo = FALSE}
dataTest <- dataTest %>%
  dplyr::select(-c("month", "quarter")) %>%
  mutate(is_mortgage = case_when(home_ownership == 'MORTGAGE' ~ 1, TRUE ~ 0),
         is_totalrevhilim_morethan_38700 = case_when(total_rev_hi_lim >= 38700 ~ 1, TRUE ~ 0),
         is_totalacc_morethan22 = case_when(total_acc >= 22 ~1, TRUE ~ 0),
         no_information_emplength = case_when(emp_length=="n/a" ~ 1, TRUE ~ 0),
         if_employment_less2years = case_when(emp_length %in% c("< 1 year","1 year", "2 years") ~1, TRUE ~ 0)
  ) %>%
  dplyr::select(-c( "purpose", "home_ownership", "inq_last_6mths_grouped", "emp_length","if_delinq_ever", highly_correlated))

dataValid <- dataValid %>%
  dplyr::select(-c("month", "quarter")) %>%
  mutate(is_mortgage = case_when(home_ownership == 'MORTGAGE' ~ 1, TRUE ~ 0),
         is_totalrevhilim_morethan_38700 = case_when(total_rev_hi_lim >= 38700 ~ 1, TRUE ~ 0),
         is_totalacc_morethan22 = case_when(total_acc >= 22 ~1, TRUE ~ 0),
         no_information_emplength = case_when(emp_length=="n/a" ~ 1, TRUE ~ 0),
         if_employment_less2years = case_when(emp_length %in% c("< 1 year","1 year", "2 years") ~1, TRUE ~ 0)
  ) %>%
  dplyr::select(-c( "purpose", "home_ownership", "inq_last_6mths_grouped", "emp_length","if_delinq_ever", highly_correlated))

dataTrainWithNewFeatures <- dataTrainWithNewFeatures %>%
  mutate(is_mortgage = case_when(home_ownership == 'MORTGAGE' ~ 1, TRUE ~ 0),
         is_totalrevhilim_morethan_38700 = case_when(total_rev_hi_lim >= 38700 ~ 1, TRUE ~ 0),
         is_totalacc_morethan22 = case_when(total_acc >= 22 ~1, TRUE ~ 0),
         no_information_emplength = case_when(emp_length=="n/a" ~ 1, TRUE ~ 0),
         if_employment_less2years = case_when(emp_length %in% c("< 1 year","1 year", "2 years") ~1, TRUE ~ 0)
  ) %>%
  dplyr::select(-c( "purpose", "home_ownership", "inq_last_6mths_grouped", "emp_length","if_delinq_ever", highly_correlated))

```

```{r another_glm_stepwise, message = FALSE, warning = FALSE, echo = TRUE, eval=FALSE}
model_glm <- glm(target~.,
                 data = dataTrainWithNewFeatures,
                 family = binomial(link = "logit"))
null_model_glm <- glm(target ~ 1, data = dataTrainWithNewFeatures, family = binomial(link = "logit"))

step_model_glm <- step(null_model_glm, scope = list(lower = null_model_glm, upper = model_glm),
                          direction = "forward")
```


```{r another_glm_predictions, message = FALSE, warning = FALSE, echo = FALSE, out.width = "50%"}
model_glm_after_step <- glm(target ~ annual_inc + revol_util + if_inq_in_last_6moths + dti + 
    tot_cur_bal + no_information_emplength + total_acc + open_acc + 
    total_rev_hi_lim + loan_amnt + if_delinq_in_last_year + 
    is_mortgage + tot_coll_amt + is_totalrevhilim_morethan_38700 + 
    if_purpose_debt_consolidation,
    data = dataTrainWithNewFeatures, family = binomial(link = "logit"))

train_predictions$model_glm_after_step_pred <- predict(model_glm_after_step, dataTrainWithNewFeatures, type = "response")
test_predictions$model_glm_after_step_pred  <- predict(model_glm_after_step, dataTest,  type = "response") 
valid_predictions$model_glm_after_step_pred <- predict(model_glm_after_step, dataValid, type = "response")

plot.roc(train_predictions$target, train_predictions$model_glm_after_step_pred, col = "red", main="ROC training data",
         percent = TRUE, print.auc = TRUE)

plot.roc(test_predictions$target, test_predictions$model_glm_after_step_pred, col = "red", main="ROC testing data",
         percent = TRUE, print.auc = TRUE)

plot.roc(valid_predictions$target, valid_predictions$model_glm_after_step_pred, col = "red", main="ROC validation data",
         percent = TRUE, print.auc = TRUE)

accuracy_glm <- show_accuracy_for_cutoffs(train = train_predictions,
                                          test = test_predictions,
                                          valid = valid_predictions,
                                          predict = "model_glm_after_step_pred",
                                          actual = "target",
                                          cutoffmax = 0.35 , cutoffmin = 0.1)

accuracy_glm$plot
accuracy_glm$data

confusionMatrix(table( as.numeric( train_predictions$model_glm_after_step_pred > 0.35 ), train_predictions$target ))
confusionMatrix(table( as.numeric( test_predictions$model_glm_after_step_pred > 0.35 ), test_predictions$target ))
confusionMatrix(table( as.numeric( valid_predictions$model_glm_after_step_pred > 0.35 ), valid_predictions$target ))

ggplot(train_predictions, aes(model_glm_after_step_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The training dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()

ggplot(test_predictions, aes(model_glm_after_step_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The testing dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()

ggplot(valid_predictions, aes(model_glm_after_step_pred, color = target ) ) +
  geom_density( size = 1 ) +
  ggtitle( "The validation dataset's Predicted Score" ) + 
  xlab("prediction")+
  scale_color_economist( name = "data", labels = c( "negative", "positive" ) ) + 
  theme_economist()
```




## ML with **caret** 


- feature selection (rfe)

- modeling

- tunning parameters 

It is important to determine the important variables first before feeding them to the ML algorithm.
For example, a good way is to use *the recursive feature elimination (RFE)* (backwards selection of predictors based on predictor importance ranking). 









<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

I think it is still exists a better way to achieve more satisfying classification results. Maybe it depends on the time samples or the definition of the target. Maybe more work is needed at creating better new features from an existing dataset. 

Session Info

```{r session_info, message = FALSE, warning = FALSE, echo = FALSE}
sessionInfo()
```

